"""
Servicio de an√°lisis de coherencia con IA avanzada usando Groq API
==================================================================
Utiliza Llama 3.1 70B para an√°lisis sem√°ntico profundo de coherencia
en exposiciones acad√©micas.

Analiza:
- Coherencia tem√°tica con las instrucciones de la asignaci√≥n
- Relevancia del contenido transcrito
- Profundidad y calidad del an√°lisis
- Estructura y organizaci√≥n del discurso
"""

from groq import Groq
from django.conf import settings
import logging
import json
import re

logger = logging.getLogger(__name__)


class AdvancedCoherenceService:
    """
    An√°lisis de coherencia con IA avanzada usando Groq API (Llama 3.1 70B)
    
    Proporciona evaluaci√≥n detallada de la coherencia entre:
    - Las instrucciones/descripci√≥n de la asignaci√≥n
    - El contenido transcrito por Whisper
    - El tema general de la exposici√≥n
    """
    
    def __init__(self):
        """Inicializa el servicio con la API key de Groq"""
        if not settings.GROQ_API_KEY:
            raise ValueError(
                "‚ö†Ô∏è GROQ_API_KEY no configurada en settings. "
                "Configura la variable de entorno GROQ_API_KEY"
            )
        
        self.client = Groq(api_key=settings.GROQ_API_KEY)
        self.config = settings.COHERENCE_CONFIG
        
        logger.info("‚úÖ AdvancedCoherenceService inicializado con Groq API")
    
    def analyze_participant_coherence(
        self,
        participant_name: str,
        transcribed_text: str,
        assignment_title: str,
        assignment_description: str
    ) -> dict:
        """
        Analiza la coherencia de un participante individual.
        
        Args:
            participant_name: Nombre/etiqueta del participante
            transcribed_text: Texto transcrito por Whisper de este participante
            assignment_title: T√≠tulo de la asignaci√≥n
            assignment_description: Descripci√≥n completa de la asignaci√≥n
        
        Returns:
            dict con:
                - coherence_score: float (0-100)
                - feedback: str (retroalimentaci√≥n detallada)
                - details: dict (desglose por criterios)
                - strengths: list (puntos fuertes)
                - improvements: list (√°reas de mejora)
        """
        # Validar entrada
        if not transcribed_text or len(transcribed_text.strip()) < 20:
            return self._insufficient_text_response(participant_name)
        
        try:
            # Construir prompt optimizado
            prompt = self._build_evaluation_prompt(
                participant_name=participant_name,
                transcribed_text=transcribed_text,
                assignment_title=assignment_title,
                assignment_description=assignment_description
            )
            
            # Llamar a Groq API
            logger.info(f"ü§ñ Analizando coherencia con Groq para: {participant_name}")
            logger.info(f"üìù Texto a analizar: {len(transcribed_text)} caracteres")
            
            response = self.client.chat.completions.create(
                model=self.config['model'],
                messages=[
                    {
                        "role": "system",
                        "content": self._get_system_prompt()
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=self.config['temperature'],
                max_tokens=self.config['max_tokens']
            )
            
            # Parsear respuesta de la IA
            ai_response = response.choices[0].message.content
            result = self._parse_ai_response(ai_response, participant_name)
            
            logger.info(
                f"‚úÖ An√°lisis completado para {participant_name}: "
                f"{result['coherence_score']:.1f}% de coherencia"
            )
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis con Groq: {str(e)}", exc_info=True)
            return self._fallback_response(participant_name, str(e))
    
    def _get_system_prompt(self) -> str:
        """Prompt del sistema que define el rol de la IA"""
        return """Eres un evaluador acad√©mico experto especializado en:
- An√°lisis de coherencia y relevancia en exposiciones orales
- Evaluaci√≥n de comprensi√≥n y profundidad de contenido
- Retroalimentaci√≥n constructiva y espec√≠fica para estudiantes

Tu objetivo es evaluar objetivamente si lo que el estudiante dijo (seg√∫n la transcripci√≥n) 
es coherente con las instrucciones de la asignaci√≥n que se le dio.

IMPORTANTE:
- S√© justo pero exigente
- Proporciona feedback espec√≠fico y √∫til
- Detecta si el estudiante comprendi√≥ realmente el tema
- Identifica si el contenido es relevante o si divaga
- Reconoce tanto fortalezas como √°reas de mejora"""
    
    def _build_evaluation_prompt(
        self,
        participant_name: str,
        transcribed_text: str,
        assignment_title: str,
        assignment_description: str
    ) -> str:
        """Construye el prompt de evaluaci√≥n con toda la informaci√≥n"""
        
        # Truncar texto si es muy largo (para no exceder l√≠mites de tokens)
        max_text_length = 4000
        if len(transcribed_text) > max_text_length:
            transcribed_text = transcribed_text[:max_text_length] + "..."
            logger.warning(f"‚ö†Ô∏è Texto truncado a {max_text_length} caracteres")
        
        return f"""
Eval√∫a la coherencia entre lo que el estudiante dijo y las instrucciones de la asignaci√≥n.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìã ASIGNACI√ìN DADA AL ESTUDIANTE
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

**T√çTULO:** {assignment_title}

**INSTRUCCIONES/DESCRIPCI√ìN:**
{assignment_description if assignment_description else "No se proporcion√≥ descripci√≥n espec√≠fica"}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üé§ LO QUE EL ESTUDIANTE DIJO (Transcripci√≥n de Whisper)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

**PARTICIPANTE:** {participant_name}

**TRANSCRIPCI√ìN:**
"{transcribed_text}"

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìä CRITERIOS DE EVALUACI√ìN
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Eval√∫a de 0-100 cada criterio:

1. **COHERENCIA TEM√ÅTICA (40%):**
   - ¬øAborda el tema/instrucciones de la asignaci√≥n?
   - ¬øSe mantiene enfocado o divaga?
   - ¬øEl contenido es pertinente?

2. **COMPRENSI√ìN Y PROFUNDIDAD (30%):**
   - ¬øDemuestra comprensi√≥n del tema?
   - ¬øIncluye detalles, ejemplos o datos?
   - ¬øEs superficial o profundo?

3. **RELEVANCIA DEL CONTENIDO (20%):**
   - ¬øLa informaci√≥n aportada es valiosa?
   - ¬øResponde a lo que se ped√≠a?
   - ¬øEvita contenido irrelevante?

4. **ESTRUCTURA Y CLARIDAD (10%):**
   - ¬øEl discurso tiene estructura l√≥gica?
   - ¬øLas ideas se expresan claramente?
   - ¬øHay fluidez en la exposici√≥n?

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìù FORMATO DE RESPUESTA REQUERIDO
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Responde EXACTAMENTE en este formato JSON (sin texto adicional):

```json
{{
  "thematic_coherence": 85.0,
  "depth_understanding": 75.0,
  "content_relevance": 90.0,
  "structure_clarity": 80.0,
  "overall_coherence": 82.5,
  "feedback": "An√°lisis breve (150-250 palabras) que explique la calificaci√≥n, destacando qu√© tan bien cumpli√≥ con las instrucciones de la asignaci√≥n.",
  "strengths": [
    "Punto fuerte espec√≠fico 1",
    "Punto fuerte espec√≠fico 2",
    "Punto fuerte espec√≠fico 3"
  ],
  "improvements": [
    "Sugerencia concreta 1",
    "Sugerencia concreta 2",
    "Sugerencia concreta 3"
  ],
  "key_concepts_covered": [
    "Concepto clave 1 mencionado",
    "Concepto clave 2 mencionado"
  ],
  "missing_elements": [
    "Elemento que falt√≥ seg√∫n las instrucciones",
    "Otro aspecto no abordado"
  ]
}}
```

IMPORTANTE: 
- S√© espec√≠fico y objetivo
- Basa tu evaluaci√≥n en la coherencia entre instrucciones y transcripci√≥n
- El feedback debe ser constructivo y √∫til para el estudiante
"""
    
    def _parse_ai_response(self, response_text: str, participant_name: str) -> dict:
        """
        Parsea la respuesta JSON de la IA y extrae las calificaciones.
        """
        try:
            # Intentar extraer JSON del texto
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            
            if not json_match:
                raise ValueError("No se encontr√≥ JSON en la respuesta")
            
            data = json.loads(json_match.group())
            
            # Extraer puntuaciones
            thematic = float(data.get('thematic_coherence', 70))
            depth = float(data.get('depth_understanding', 70))
            relevance = float(data.get('content_relevance', 70))
            structure = float(data.get('structure_clarity', 70))
            
            # Calcular score final ponderado
            overall_score = (
                thematic * 0.40 +
                depth * 0.30 +
                relevance * 0.20 +
                structure * 0.10
            )
            
            # Validar que est√© en rango 0-100
            overall_score = max(0, min(100, overall_score))
            
            return {
                'coherence_score': round(overall_score, 1),
                'feedback': data.get('feedback', 'An√°lisis completado por IA'),
                'details': {
                    'thematic_coherence': thematic,
                    'depth_understanding': depth,
                    'content_relevance': relevance,
                    'structure_clarity': structure
                },
                'strengths': data.get('strengths', []),
                'improvements': data.get('improvements', []),
                'key_concepts_covered': data.get('key_concepts_covered', []),
                'missing_elements': data.get('missing_elements', []),
                'ai_powered': True,
                'participant_name': participant_name
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Error parseando JSON: {e}")
            logger.debug(f"Respuesta recibida: {response_text[:500]}")
            
            # Intentar extraer score del texto plano
            score_match = re.search(r'(\d+\.?\d*)\s*[%/]', response_text)
            score = float(score_match.group(1)) if score_match else 70.0
            
            return {
                'coherence_score': min(100, score),
                'feedback': response_text[:500],
                'details': {
                    'thematic_coherence': score,
                    'depth_understanding': score,
                    'content_relevance': score,
                    'structure_clarity': score
                },
                'strengths': ['An√°lisis completado'],
                'improvements': ['Ver feedback para detalles'],
                'ai_powered': True,
                'participant_name': participant_name
            }
        
        except Exception as e:
            logger.error(f"‚ùå Error inesperado parseando respuesta: {e}")
            raise
    
    def _insufficient_text_response(self, participant_name: str) -> dict:
        """Respuesta cuando el texto es insuficiente para an√°lisis"""
        return {
            'coherence_score': 0.0,
            'feedback': (
                f"‚ö†Ô∏è {participant_name} tiene texto insuficiente para an√°lisis "
                f"(menos de 20 caracteres). Esto puede indicar que no particip√≥ "
                f"verbalmente o que la transcripci√≥n fall√≥."
            ),
            'details': {
                'thematic_coherence': 0.0,
                'depth_understanding': 0.0,
                'content_relevance': 0.0,
                'structure_clarity': 0.0
            },
            'strengths': [],
            'improvements': ['Participar m√°s activamente en la exposici√≥n oral'],
            'key_concepts_covered': [],
            'missing_elements': ['Contenido verbal'],
            'ai_powered': False,
            'participant_name': participant_name
        }
    
    def _fallback_response(self, participant_name: str, error_message: str) -> dict:
        """Respuesta de fallback cuando falla la API"""
        return {
            'coherence_score': 0.0,
            'feedback': (
                f"‚ùå No se pudo analizar la coherencia de {participant_name} "
                f"con IA avanzada. Error: {error_message}. "
                f"Por favor, revisa la configuraci√≥n de GROQ_API_KEY."
            ),
            'details': {
                'thematic_coherence': 0.0,
                'depth_understanding': 0.0,
                'content_relevance': 0.0,
                'structure_clarity': 0.0
            },
            'strengths': [],
            'improvements': [],
            'key_concepts_covered': [],
            'missing_elements': [],
            'ai_powered': False,
            'participant_name': participant_name,
            'error': error_message
        }
    
    def batch_analyze(self, participants_data: list, assignment_info: dict) -> list:
        """
        Analiza m√∫ltiples participantes en batch.
        
        Args:
            participants_data: Lista de dicts con 'name' y 'transcription'
            assignment_info: Dict con 'title' y 'description'
        
        Returns:
            Lista de resultados de an√°lisis
        """
        results = []
        
        assignment_title = assignment_info.get('title', 'Sin t√≠tulo')
        assignment_description = assignment_info.get('description', '')
        
        for participant in participants_data:
            result = self.analyze_participant_coherence(
                participant_name=participant['name'],
                transcribed_text=participant['transcription'],
                assignment_title=assignment_title,
                assignment_description=assignment_description
            )
            results.append(result)
        
        return results
    
    @staticmethod
    def is_available() -> bool:
        """Verifica si el servicio est√° disponible (API key configurada)"""
        return bool(settings.GROQ_API_KEY)
