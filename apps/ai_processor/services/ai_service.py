# apps/ai_processor/services/ai_service.py
import logging
from django.utils import timezone
from .transcription_service import TranscriptionService
from .face_detection_service import FaceDetectionService
from .liveness_detection_service import LivenessDetectionService
from .coherence_analyzer import CoherenceAnalyzer
from .audio_segmentation_service import AudioSegmentationService

logger = logging.getLogger(__name__)

class AIService:
    def __init__(self):
        self.transcription_service = TranscriptionService()
        self.face_detection_service = FaceDetectionService(
            tolerance=0.6,  # Sensibilidad de comparaci√≥n de rostros
            sample_rate=30  # Analizar 1 frame por segundo (asumiendo 30 FPS)
        )
        self.liveness_detection_service = LivenessDetectionService()
        self.coherence_analyzer = CoherenceAnalyzer()  # NUEVO
        
        # Servicio de segmentaci√≥n de audio
        recommended_strategy = AudioSegmentationService.get_recommended_strategy()
        self.audio_segmentation_service = AudioSegmentationService(strategy=recommended_strategy)
        logger.info(f"üé§ Audio segmentation strategy: {recommended_strategy}")
    
    def analyze_presentation(self, presentation):
        """
        An√°lisis completo de una presentaci√≥n con evaluaci√≥n individual
        """
        try:
            # Actualizar estado a procesando
            presentation.status = 'PROCESSING'
            presentation.save()
            
            # Funci√≥n helper para reportar progreso
            def report_progress(progress, step):
                from django.core.cache import cache
                cache.set(f'presentation_progress_{presentation.id}', {
                    'status': 'PROCESSING',
                    'progress': progress,
                    'step': step
                }, timeout=3600)
            
            video_path = presentation.video_file.path
            
            # 1. An√°lisis de liveness (video en vivo vs pregrabado)
            report_progress(15, 'Analizando autenticidad del video...')
            logger.info(f"üé• Iniciando an√°lisis de liveness para presentaci√≥n {presentation.id}")
            liveness_result = self.liveness_detection_service.analyze_video(video_path)
            
            # Guardar resultados de liveness
            if liveness_result['success']:
                presentation.is_live_recording = liveness_result['is_live']
                presentation.liveness_score = liveness_result['liveness_score']
                presentation.liveness_confidence = liveness_result['confidence']
                presentation.recording_type = liveness_result['recording_type']
            
            # 2. Detecci√≥n de rostros y an√°lisis de participaci√≥n
            report_progress(30, 'Detectando rostros y participantes...')
            logger.info(f"üë• Iniciando detecci√≥n de rostros para presentaci√≥n {presentation.id}")
            face_analysis = self.face_detection_service.process_video(video_path, presentation_id=presentation.id)
            
            # Guardar datos de participaci√≥n b√°sicos
            presentation.participation_data = face_analysis
            
            # 3. Transcripci√≥n completa del video
            report_progress(50, 'Transcribiendo audio con Whisper...')
            logger.info(f"üé§ Iniciando transcripci√≥n completa para presentaci√≥n {presentation.id}")
            transcription_result = self.transcription_service.transcribe_video(video_path)
            
            # Guardar transcripci√≥n completa
            presentation.transcription_text = transcription_result['full_text']
            presentation.transcription_segments = transcription_result['segments']
            presentation.audio_duration = transcription_result['duration']
            presentation.transcription_completed_at = timezone.now()
            
            # ===== VALIDACIONES CR√çTICAS =====
            # Verificar si hay audio
            has_audio = bool(transcription_result['full_text'] and transcription_result['full_text'].strip())
            has_face = face_analysis['success'] and face_analysis.get('total_participants', 0) > 0
            
            # CASO 1: No hay audio - ERROR CR√çTICO
            if not has_audio:
                error_msg = "‚ùå No se detect√≥ audio en el video. Por favor, verifica que tu micr√≥fono est√© funcionando y graba nuevamente."
                logger.error(f"Sin audio detectado en presentaci√≥n {presentation.id}")
                presentation.status = 'FAILED'
                presentation.ai_feedback = error_msg
                presentation.save()
                
                from django.core.cache import cache
                cache.set(f'presentation_progress_{presentation.id}', {
                    'status': 'FAILED',
                    'progress': 0,
                    'step': 'Sin audio detectado',
                    'error': error_msg
                }, timeout=3600)
                
                return False
            
            # CASO 2: No hay cara - ERROR CR√çTICO (solo si tampoco hay audio)
            # Pero si hay audio sin cara, continuar con an√°lisis de audio √∫nicamente
            if not has_face:
                logger.warning(f"‚ö†Ô∏è No se detectaron rostros en presentaci√≥n {presentation.id}")
                # Marcar que no hay rostros pero continuar procesamiento
                presentation.participation_data['no_face_detected'] = True
                presentation.participation_data['warning'] = "No se detectaron rostros en el video"
            
            # 4. AN√ÅLISIS INDIVIDUAL DE COHERENCIA
            report_progress(70, 'Analizando coherencia individual...')
            logger.info(f"üß† Iniciando an√°lisis individual de coherencia")
            
            # Determinar si hay rostros detectados
            has_participants = face_analysis['success'] and face_analysis.get('participants') and len(face_analysis['participants']) > 0
            
            if has_participants:
                # CASO NORMAL: Hay rostros detectados
                logger.info(f"‚úÖ {len(face_analysis['participants'])} participante(s) detectado(s)")
                
                # Preparar tema y descripci√≥n
                tema = presentation.assignment.title if presentation.assignment else "Tema general"
                descripcion_tema = presentation.assignment.description if presentation.assignment else ""
                
                # Asignar transcripci√≥n completa si solo hay 1 participante
                # Para m√∫ltiples, idealmente se deber√≠a segmentar por tiempo
                participants_data = self._prepare_participants_data(
                    face_analysis['participants'],
                    transcription_result,
                    video_path
                )
                
                # Obtener puntaje m√°ximo de la asignaci√≥n (default 20)
                max_score = float(presentation.assignment.max_score) if presentation.assignment else 20.0
                
                # Obtener assignment para configuraci√≥n de IA
                assignment = presentation.assignment
                if assignment:
                    logger.info(f"ÔøΩ Assignment identificado: {assignment.title}")
                
                # Analizar coherencia individual con assignment
                coherence_results = self.coherence_analyzer.analizar_grupo(
                    participants_data,
                    tema,
                    descripcion_tema,
                    max_score=max_score,  # Pasar puntaje m√°ximo
                    assignment=assignment  # Pasar assignment completo para configuraci√≥n de estrictez
                )
                
                # Guardar participantes individuales en la BD
                self._save_participants(presentation, coherence_results)
                
                # Calcular score promedio de coherencia
                avg_coherence = sum(r['nota_coherencia'] for r in coherence_results) / len(coherence_results)
                presentation.ai_score = avg_coherence
                
            else:
                # CASO ESPECIAL: No hay rostros pero hay audio
                # An√°lisis solo por audio (voz en off)
                logger.warning("‚ö†Ô∏è No se detectaron rostros - An√°lisis solo por audio")
                
                # Preparar tema y descripci√≥n
                tema = presentation.assignment.title if presentation.assignment else "Tema general"
                descripcion_tema = presentation.assignment.description if presentation.assignment else ""
                max_score = float(presentation.assignment.max_score) if presentation.assignment else 20.0
                
                # Obtener assignment para configuraci√≥n de IA
                assignment = presentation.assignment
                
                # Crear un participante virtual "Sin Rostro Detectado"
                participants_data = [{
                    'etiqueta': 'Sin Rostro Detectado',
                    'texto_transcrito': transcription_result['full_text'],
                    'tiempo_participacion': transcription_result['duration'],
                    'foto_url': None
                }]
                
                # Analizar coherencia del audio con assignment
                coherence_results = self.coherence_analyzer.analizar_grupo(
                    participants_data,
                    tema,
                    descripcion_tema,
                    max_score=max_score,
                    assignment=assignment
                )
                
                # Marcar que no hay rostro en el resultado
                coherence_results[0]['sin_rostro'] = True
                coherence_results[0]['observacion'] += " ‚ö†Ô∏è NOTA: No se detect√≥ ning√∫n rostro en el video, calificaci√≥n basada √∫nicamente en el an√°lisis de audio."
                
                # Guardar participante virtual en la BD
                self._save_participants(presentation, coherence_results)
                
                # Calcular score
                avg_coherence = coherence_results[0]['nota_coherencia']
                presentation.ai_score = avg_coherence
            
            # 5. Generar feedback detallado
            report_progress(90, 'Generando retroalimentaci√≥n...')
            presentation.ai_feedback = self.generate_feedback(
                transcription_result,
                avg_coherence if face_analysis['participants'] else presentation.ai_score,
                face_analysis,
                liveness_result,
                coherence_results
            )
            
            # Actualizar estado
            presentation.status = 'ANALYZED'
            presentation.analyzed_at = timezone.now()
            presentation.save()
            
            report_progress(100, 'An√°lisis completado ‚úÖ')
            logger.info(f"‚úÖ An√°lisis completado para presentaci√≥n {presentation.id}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis IA: {str(e)}", exc_info=True)
            presentation.ai_feedback = f"Error en an√°lisis: {str(e)}"
            presentation.status = 'FAILED'
            presentation.save()
            return False
    
    def analyze_coherence(self, transcription, topic_description):
        """
        An√°lisis b√°sico de coherencia tem√°tica (fallback)
        """
        if not transcription or not topic_description:
            return 0
        
        # Convertir a min√∫sculas y dividir en palabras
        topic_words = set(topic_description.lower().split())
        transcription_words = set(transcription.lower().split())
        
        # Calcular intersecci√≥n
        common_words = topic_words.intersection(transcription_words)
        
        if len(topic_words) == 0:
            return 0
        
        # Calcular porcentaje de coincidencia
        coherence_score = (len(common_words) / len(topic_words)) * 100
        
        # Limitar a 100%
        return min(100, coherence_score)
    
    def _prepare_participants_data(self, participants, transcription_result, video_path):
        """
        Prepara datos de participantes para an√°lisis de coherencia.
        
        Utiliza segmentaci√≥n de audio para asignar transcripciones exactas
        a cada participante seg√∫n qui√©n habla cu√°ndo.
        """
        full_text = transcription_result['full_text']
        total_duration = transcription_result['duration']
        
        # Si no hay transcripci√≥n, retornar datos b√°sicos
        if not full_text or not full_text.strip():
            return [{
                'etiqueta': p['id'],
                'texto_transcrito': '',
                'tiempo_participacion': p['time_seconds'],
                'foto_url': p.get('photo')
            } for p in participants]
        
        # Si solo hay 1 participante, darle toda la transcripci√≥n
        if len(participants) == 1:
            return [{
                'etiqueta': participants[0]['id'],
                'texto_transcrito': full_text,
                'tiempo_participacion': participants[0]['time_seconds'],
                'foto_url': participants[0].get('photo')
            }]
        
        # Para m√∫ltiples participantes: segmentar audio por hablante
        logger.info(f"üé§ Segmentando audio para {len(participants)} participantes")
        
        try:
            # Crear estructura de datos compatible con AudioSegmentationService
            participants_for_segmentation = []
            for p in participants:
                participants_for_segmentation.append({
                    'participant_id': p['id'],
                    'name': p['id'],  # Usar 'id' como nombre (ej: "Persona 1")
                    'appearances': p.get('appearances', []),
                    'total_participation_time': p['time_seconds']
                })
            
            # Ejecutar segmentaci√≥n
            segmented_participants = self.audio_segmentation_service.segment_audio_by_participants(
                video_path,
                participants_for_segmentation,
                transcription_result
            )
            
            # Convertir a formato esperado por coherence_analyzer
            participants_data = []
            for seg_participant in segmented_participants:
                # Buscar participante original para obtener foto
                original_participant = next(
                    (p for p in participants if p['id'] == seg_participant['participant_id']),
                    None
                )
                
                participants_data.append({
                    'etiqueta': seg_participant['participant_id'],
                    'texto_transcrito': seg_participant.get('transcription', ''),
                    'tiempo_participacion': seg_participant.get('total_participation_time', 0),
                    'foto_url': original_participant.get('photo') if original_participant else None,
                    'speech_segments': seg_participant.get('speech_segments', []),
                    'time_segments': original_participant.get('time_segments', []) if original_participant else []
                })
            
            logger.info(f"‚úÖ Segmentaci√≥n completada: {len(participants_data)} participantes con transcripciones asignadas")
            return participants_data
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error en segmentaci√≥n de audio: {str(e)}. Usando m√©todo proporcional.")
            
            # Fallback: distribuci√≥n proporcional (m√©todo antiguo)
            participants_data = []
            palabras = full_text.split()
            
            for participant in participants:
                tiempo_porcentaje = participant['percentage'] / 100
                num_palabras = int(len(palabras) * tiempo_porcentaje)
                texto = ' '.join(palabras[:max(num_palabras, 20)])  # M√≠nimo 20 palabras
                
                participants_data.append({
                    'etiqueta': participant['id'],
                    'texto_transcrito': texto,
                    'tiempo_participacion': participant['time_seconds'],
                    'foto_url': participant.get('photo'),
                    'time_segments': participant.get('time_segments', [])
                })
            
            return participants_data
    
    def _save_participants(self, presentation, coherence_results):
        """
        Guarda los resultados individuales de participantes en la BD
        """
        from apps.presentaciones.models import Participant
        
        # Eliminar participantes anteriores si existen
        presentation.participants.all().delete()
        
        # Crear nuevos participantes
        for resultado in coherence_results:
            # Buscar la foto si existe
            photo_path = resultado.get('foto_url')
            
            # Preparar feedback de IA
            ai_feedback_text = f"""**An√°lisis de Coherencia:**
{resultado['observacion']}

**M√©tricas Detectadas:**
- Coherencia sem√°ntica: {resultado['coherencia_semantica']}%
- Palabras clave encontradas: {resultado['puntaje_palabras_clave']}%
- Profundidad del contenido: {resultado['puntaje_profundidad']}%
- Tiempo de participaci√≥n: {resultado['tiempo_participacion']}s ({resultado['porcentaje_tiempo']}%)

**Palabras clave identificadas:** {', '.join(resultado['palabras_clave_encontradas'][:10]) if resultado['palabras_clave_encontradas'] else 'Ninguna'}

**Nivel de coherencia:** {resultado['nivel']}
"""
            
            # Si hay feedback avanzado de IA (Groq), usarlo
            if 'feedback_ia_avanzada' in resultado:
                ai_feedback_text = resultado['feedback_ia_avanzada']
            
            Participant.objects.create(
                presentation=presentation,
                label=resultado['etiqueta'],
                photo=photo_path,  # Guardar ruta de la foto
                participation_time=resultado['tiempo_participacion'],
                time_percentage=resultado['porcentaje_tiempo'],
                time_segments=resultado.get('time_segments', []),  # Guardar segmentos de tiempo
                transcription=resultado['texto_transcrito'],
                word_count=resultado['palabras_totales'],
                semantic_coherence=resultado['coherencia_semantica'],
                keywords_score=resultado['puntaje_palabras_clave'],
                depth_score=resultado['puntaje_profundidad'],
                coherence_score=resultado['nota_coherencia'],
                contribution_percentage=resultado['porcentaje_aporte_normalizado'],
                ai_grade=resultado['calificacion_final'],  # Calificaci√≥n de IA
                ai_feedback=ai_feedback_text,  # Feedback de IA
                coherence_level=resultado['nivel'],
                observations=resultado['observacion'],
                keywords_found=resultado['palabras_clave_encontradas']
            )
        
        logger.info(f"üíæ Guardados {len(coherence_results)} participantes en la BD")
    
    def analyze_participation(self, video_path):
        """
        An√°lisis REAL de participaci√≥n mediante detecci√≥n de rostros
        
        Detecta y compara rostros en el video para:
        - Identificar participantes √∫nicos (Persona 1, Persona 2, etc.)
        - Medir tiempo de pantalla de cada participante
        - Calcular porcentajes de participaci√≥n
        - Evaluar equidad en la distribuci√≥n del tiempo
        
        Returns:
            dict: Datos de participaci√≥n con score de equidad
        """
        logger.info(f"üé≠ Iniciando an√°lisis de participaci√≥n con detecci√≥n de rostros")
        
        try:
            # Usar el servicio de detecci√≥n de rostros
            result = self.face_detection_service.process_video(video_path)
            
            if not result['success']:
                logger.error(f"Error en detecci√≥n de rostros: {result.get('error', 'Unknown error')}")
                # Fallback a datos b√°sicos en caso de error
                return {
                    'score': 0,
                    'participants': [],
                    'error': result.get('error', 'Error en detecci√≥n de rostros'),
                    'total_participants': 0
                }
            
            # Formatear participantes para compatibilidad con c√≥digo existente
            formatted_participants = []
            for p in result['participants']:
                formatted_participants.append({
                    'id': p['id'],
                    'percentage': round(p['percentage'], 1),
                    'time': p['time_formatted'],
                    'time_seconds': p['time_seconds'],
                    'appearances': p['appearances_count']
                })
            
            logger.info(f"‚úÖ Participaci√≥n analizada: {result['total_participants']} participantes, Score: {result['score']:.1f}")
            
            return {
                'score': result['score'],
                'participants': formatted_participants,
                'total_participants': result['total_participants'],
                'frames_analyzed': result.get('frames_analyzed', 0),
                'faces_detected': result.get('faces_detected', 0)
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error cr√≠tico en an√°lisis de participaci√≥n: {str(e)}", exc_info=True)
            # Retornar estructura de error
            return {
                'score': 0,
                'participants': [],
                'error': str(e),
                'total_participants': 0
            }
    
    def generate_feedback(self, transcription_result, coherence_score, participation_data, liveness_result, coherence_results=[]):
        """
        Genera feedback autom√°tico basado en el an√°lisis
        Incluye informaci√≥n detallada individual de cada participante
        """
        feedback = f"""
ü§ñ **An√°lisis Autom√°tico Completado**

"""
        
        # Informaci√≥n de autenticidad (liveness)
        if liveness_result.get('success', False):
            feedback += f"""üé• **Autenticidad de la Grabaci√≥n:**
- Tipo: {liveness_result['type_display']}
- Score de Liveness: {liveness_result['liveness_score']:.1f}/100
- Confianza: {liveness_result['confidence']:.1f}%

"""
            if liveness_result['recording_type'] == 'LIVE':
                feedback += "‚úÖ Video grabado en vivo detectado\n\n"
            elif liveness_result['recording_type'] == 'LIKELY_LIVE':
                feedback += "‚ö†Ô∏è Video probablemente grabado en vivo\n\n"
            elif liveness_result['recording_type'] == 'LIKELY_RECORDED':
                feedback += "‚ö†Ô∏è Video probablemente pregrabado\n\n"
            else:
                feedback += "‚ùå Video pregrabado detectado\n\n"
        
        feedback += f"""üìù **Transcripci√≥n:**
- Texto transcrito: {len(transcription_result['full_text'])} caracteres
- Duraci√≥n total: {transcription_result['duration']:.1f} segundos
- Segmentos identificados: {len(transcription_result['segments'])}

"""
        
        # NUEVO: Evaluaci√≥n individual de participantes
        if coherence_results:
            # Verificar si es caso sin rostro
            is_no_face_case = len(coherence_results) == 1 and coherence_results[0].get('sin_rostro', False)
            
            if is_no_face_case:
                feedback += f"""‚ö†Ô∏è **AN√ÅLISIS SIN ROSTRO DETECTADO**\n\n"""
                feedback += f"""**IMPORTANTE:** No se detect√≥ ning√∫n rostro en el video. La calificaci√≥n est√° basada √∫nicamente en el an√°lisis del audio transcrito.\n\n"""
            else:
                feedback += f"""üìä **EVALUACI√ìN INDIVIDUAL - {len(coherence_results)} Participantes**\n\n"""
            
            # Ordenar por calificaci√≥n (mayor a menor)
            coherence_results_sorted = sorted(coherence_results, key=lambda x: x['calificacion_final'], reverse=True)
            
            for idx, resultado in enumerate(coherence_results_sorted, 1):
                # Icono diferente si no hay rostro
                icon = "üé§" if resultado.get('sin_rostro', False) else "üë§"
                
                feedback += f"""**{idx}. {icon} {resultado['etiqueta']}** - {resultado['nivel']}
   üìù Calificaci√≥n: **{resultado['calificacion_final']}/20**
   ‚è±Ô∏è  Tiempo: {resultado['porcentaje_tiempo']:.1f}%
   üìà Aporte: {resultado['porcentaje_aporte_normalizado']:.1f}%
   üéØ Coherencia: {resultado['nota_coherencia']:.1f}/100
   üí¨ Palabras: {resultado['palabras_totales']}
   
   **Desglose:**
   ‚Ä¢ Coherencia sem√°ntica: {resultado['coherencia_semantica']:.1f}/100
   ‚Ä¢ Palabras clave: {resultado['puntaje_palabras_clave']:.1f}/100
   ‚Ä¢ Profundidad: {resultado['puntaje_profundidad']:.1f}/100
   
   **Observaci√≥n:** {resultado['observacion']}
   
"""
                if resultado['palabras_clave_encontradas']:
                    palabras = ', '.join(resultado['palabras_clave_encontradas'][:5])
                    feedback += f"   üîë Palabras clave: {palabras}\n\n"
            
            # Estad√≠sticas del grupo
            import numpy as np
            promedio_coherencia = np.mean([r['nota_coherencia'] for r in coherence_results])
            promedio_calificacion = np.mean([r['calificacion_final'] for r in coherence_results])
            
            feedback += f"""**üìä Estad√≠sticas del Grupo:**
   ‚Ä¢ Coherencia promedio: {promedio_coherencia:.1f}/100
   ‚Ä¢ Calificaci√≥n promedio: {promedio_calificacion:.1f}/20
   ‚Ä¢ Participantes detectados: {len(coherence_results)}
   
"""
            
            # An√°lisis de equidad
            desviacion_tiempo = np.std([r['porcentaje_tiempo'] for r in coherence_results])
            if desviacion_tiempo < 10:
                feedback += "   ‚Ä¢ ‚úÖ Participaci√≥n muy equilibrada en tiempo\n"
            elif desviacion_tiempo < 20:
                feedback += "   ‚Ä¢ ‚ö†Ô∏è  Participaci√≥n moderadamente equilibrada\n"
            else:
                feedback += "   ‚Ä¢ ‚ùå Participaci√≥n desigual - revisar distribuci√≥n\n"
            
            feedback += "\n"
        
        else:
            # Si no hay an√°lisis individual, usar el global
            feedback += f"""üìä **Coherencia Tem√°tica General: {coherence_score:.1f}/100**
{" ‚úÖ Excelente coherencia con el tema" if coherence_score >= 80 else
 " ‚ö†Ô∏è Coherencia moderada con el tema" if coherence_score >= 60 else
 " ‚ùå Baja coherencia con el tema asignado"}

"""
        
        # Informaci√≥n de participaci√≥n (resumen)
        total_participants = participation_data.get('total_participants', 0)
        no_face_detected = participation_data.get('no_face_detected', False)
        
        if total_participants > 0:
            feedback += f"""üé≠ **Detecci√≥n de Rostros:**
- Participantes √∫nicos: {total_participants}
- Frames analizados: {participation_data.get('frames_analyzed', 0)}
- Rostros detectados: {participation_data.get('faces_detected', 0)}

"""
        else:
            feedback += """‚ö†Ô∏è **No se detectaron rostros en el video**

**Posibles causas:**
‚Ä¢ C√°mara apagada o cubierta
‚Ä¢ Participante fuera del cuadro
‚Ä¢ Iluminaci√≥n muy baja
‚Ä¢ Calidad de video muy baja

**Recomendaci√≥n:** Para una evaluaci√≥n completa, aseg√∫rate de que tu rostro sea visible durante la grabaci√≥n.

"""
        
        # Recomendaciones generales
        feedback += f"""üí° **Recomendaciones Generales:**
"""
        
        if coherence_results:
            # Recomendaciones basadas en an√°lisis individual
            notas_bajas = [r for r in coherence_results if r['calificacion_final'] < 11]
            if notas_bajas:
                feedback += f"‚Ä¢ {len(notas_bajas)} participante(s) necesita(n) mejorar la coherencia con el tema\n"
            
            max_diff_tiempo = max([r['porcentaje_tiempo'] for r in coherence_results]) - min([r['porcentaje_tiempo'] for r in coherence_results])
            if max_diff_tiempo > 30:
                feedback += "‚Ä¢ Buscar mayor equidad en la distribuci√≥n del tiempo entre participantes\n"
        
        if transcription_result['duration'] < 60:
            feedback += "‚Ä¢ Considerar extender la duraci√≥n de la presentaci√≥n\n"
        elif transcription_result['duration'] > 600:
            feedback += "‚Ä¢ La presentaci√≥n es muy larga, intenta ser m√°s conciso\n"
        
        if total_participants == 0:
            feedback += "‚Ä¢ Aseg√∫rate de que la c√°mara est√© encendida y los participantes visibles\n"
        
        return feedback
